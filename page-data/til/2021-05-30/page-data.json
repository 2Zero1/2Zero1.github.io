{"componentChunkName":"component---src-templates-blog-post-js","path":"/til/2021-05-30/","result":{"data":{"site":{"siteMetadata":{"title":"Zero1's Note","author":"Zero1","siteUrl":"https://github.com/2Zero1","comment":{"disqusShortName":"2Zero1","utterances":"2Zero1 Blog"},"sponsor":{"buyMeACoffeeId":"jbee"}}},"markdownRemark":{"id":"aa4599fa-72a0-5fe1-b0e6-feea800dadba","excerpt":"fact 스크래퍼 만들기 목표: 같은 목적을 가진 다른…","html":"<ol>\n<li>\n<p>fact</p>\n<ul>\n<li>\n<p>스크래퍼 만들기</p>\n<ol>\n<li>목표: 같은 목적을 가진 다른 2개의 사이트로부터 게시된 글들을 스크래핑 하기</li>\n<li>\n<p>내가 세운 요구사항</p>\n<ul>\n<li>정해진 형태로 스크래핑해온 데이터를 반환</li>\n<li>해당 사이트의 모든 글들을 스크래핑 하기</li>\n</ul>\n</li>\n<li>\n<p>만난 문제점들과 해결 방법</p>\n<ul>\n<li>\n<p>각각의 사이트에서 보여지는 글들은 동일한 정보를 갖고 있지만, 서로 다른 태그 또는 구성으로 구성되어져있었음</p>\n<ul>\n<li>해결 방법: 각 사이트는 같은 형식으로 데이터를 뿌려줬기 때문에 사이트마다 필요한 정보를 가져오는 구현을 다르게 처리하고 격리하였음.(전략패턴)</li>\n<li>또 각 사이트로부터 가져온 링크에 접속해서 내가 수집하려는 형태의 글(내가 수집하려는 페이지는 게시물페이지였음. 하지만 이것 이외에 메인 페이지나 필요없는 광고 페이지가 있었음.)인지 확인한 후에 수집을 진행하였음. 파악하는 방법으로, head에 특정 메타 태그가 존재하는지 확인 하였는데, 이것도 각 페이지별로 다르게 구현하여 구현하였음.</li>\n</ul>\n</li>\n<li>\n<p>해당 사이트의 모든 글들을 스크래핑하는 목표를 갖고 있어, 모든 페이지에서 모든 뉴스링크를 수집하고, 게시물로 부터 정보를 습득하는 재귀적인 처리가 필요했음</p>\n<ul>\n<li>페이지에 들어가면 LinkCollector 객체로 링크를 수집하고 queue에 쌓고, Parser를 이용해 파싱하도록 병렬 처리하도록 계획하였음.</li>\n<li>privider consumer인 멀티스레드 패턴을 사용하여 스크래퍼 객체가 페이지를 요청한 후에 LinkCollecetor와 Parser에게 각각 병렬로 일을 처리하도록 위임하는 재귀적 설계를 구상 하였음</li>\n<li>하지만 시간 부족으로 구현은 못했음.</li>\n<li>또 재귀적으로 계속 탐색하다 보면 co-recursive 하게 되어 언제 끝날지 모르는 문제가 있긴 했음..</li>\n</ul>\n</li>\n<li>\n<p>문서를 탐색하기 위해 파서를 만들어야 했음.</p>\n<ul>\n<li>일단 먼저 cheerio를 사용하여 문서를 분석하였음.</li>\n<li>하지만 이후에 내가 만든 parser를 사용할 수 있도록 격리하는것으로 계획하였음.</li>\n<li>하지만 시간이 부족해서 하진 못했음.</li>\n</ul>\n</li>\n<li>\n<p>병렬 처리가 필요한 부분이 있었음. 한번에 너무 많은 양의 요청을 하다보니 터지는 문제가 발생 하였음</p>\n<ul>\n<li>병렬 처리는 위와 같은 문제가 있어 일단 모두 동시성 프로그래밍으로 진행 하였고, 해당 페이지로부터 모든 링크를 가져와 요청을 하려 했지만 너무 느렸음.</li>\n<li>그래도 promise.all을 사용하려 했지만 링크가 너무 많아서 터지는 문제가 발생함.</li>\n<li>그래서 나눠서 처리하려고 했지만 실제로 하진 못했음.. 사실 해놓고 테스트 하다가 좀 고쳐서 날라가버렸음 …</li>\n<li>내가 생각했던 방식으론 프로미스 all을 특정 개수만큼 나눤 배열에 담고, 시간 지연을 두고 요청을 하는 방법이었음.</li>\n</ul>\n</li>\n<li>모든 페이지에서 링크를 수집하다가 잘못된 링크(잘못된 문자, url 형태가 아닌 link)를 만나면 axios 에서 에러를 뿜고 죽어버리는 문제가 있었음.</li>\n</ul>\n</li>\n</ol>\n</li>\n</ul>\n</li>\n<li>\n<p>feelings</p>\n<ul>\n<li>처음으로 스크래퍼를 만들어봤다. 쉽진 않았지만 재미있었다.</li>\n<li>테스트를 잘 작성하기 위해선(tdd를 잘하기 위해선) test 프레임워크를 잘 이해할 필요가 있다고 느꼈다.</li>\n<li>배운것을 적용해보려고 했는데 구현하기 바쁘다 보니 뒤죽박죽 적용한곳과 못한곳이 있었다.. ㅎㅎ</li>\n</ul>\n</li>\n<li>\n<p>finding</p>\n<ol>\n<li>\n<p>크롤러와 스크래퍼의 차이</p>\n<ul>\n<li>scrapping: Web scraping a web page involves fetching it and extracting from it</li>\n<li>web crawling: web crawling is a main component of web scraping</li>\n</ul>\n</li>\n<li>promise.all은 pool 설정이 불가능하다. 어떻게 최대 생성 스레드 pool을 생성해놓고 문제가 발생하지 않게 고민해 본 결과 promise.all을 쓰기 위해서 내가 구현해야된다는걸 알게 되었다. 분명 다른걸 사용하면 있을탠데… 더 찾아봐야겠다.</li>\n</ol>\n</li>\n<li>\n<p>Future Action Plan</p>\n<ul>\n<li>위에서 내가 부족했던것과 못했던 부분들을 추가적으로 적용해야겠다.</li>\n<li>테스트 프레임워크 문서를 더 읽고 사용법을 이해해야겠다.</li>\n<li>병렬 프로그래밍 공부좀 해야겠다.</li>\n</ul>\n</li>\n</ol>","frontmatter":{"title":"2021-05-30 TIL","date":"May 30, 2021"}}},"pageContext":{"slug":"/til/2021-05-30/","previous":{"fields":{"slug":"/til/2021-05-26/"},"frontmatter":{"title":"2021-05-27 TIL","category":"TIL"}},"next":{"fields":{"slug":"/study/그림으로 배우는 HTTP & Network Basic2/"},"frontmatter":{"title":"그림으로 배우는 HTTP & Network basic 2","category":"study"}}}},"staticQueryHashes":["3128451518","96099027"]}